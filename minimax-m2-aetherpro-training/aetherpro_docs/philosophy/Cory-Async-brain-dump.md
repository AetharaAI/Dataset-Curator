Since you are in async event driven arcitecture mode (internal attention from focusing on this task) I want to create a "dumb" pure python asyncio event driven kernel with an internal event bus that pubs and subs registered events, with strategic tiered processing and messaging queues for parallel task execution, sort of like a Linux kernel runtime. Linux doens't care what you software you install, what hardware or devices are inthe system, it just manages micro parallel processes and daemons, manages scheduler, etc. I know we're still in the Lotus repo, but I want you to just create a "side-project" directory for this. Imagine you just bought a computer, it has Linux or windows, OS doesn't matter, all the OS does is make the software and hardware easliy accessible to the 70% of computer owners/users who are button clickers and have to have a pretty UI with that buried setting tab for some thing that make the other thing work. This is how most people think. Now we have a brand new machine, we fire it up and first thing we have to do is what? Install all the programs and repo's that power user need( i am opening a terminal as soon as the UI hits the screen) now for some of the heavy apps and repo's we just installed now we got to restart the machine or shell depending on what we did for setting to take effect(refactoring code or adding a feature, we restart so the code picks up the changes) i am getting somewhere, i see you yawning like the student trying to teach the Professor and the Professor is bored, lol. When the system is back on all those new apps and dev essentials are still there, right? Now translate that to my async kernel like runtime. I want to build a solid, reproducible, add anything, or uninstall anything and the kernel still works, no matter what. So like in Lotus, i think we have some sort of module discovery system at boot, but you see how brittle that is? Or did we do something wrong? The concept of modules is correct to a point. Me or you or another dev can add and suntract from a system like that, but the 70%? Where's the magic buttons? I don't know what a terminal command is or how to run a terminal, how do i add new stuff to the system? Or remove something we dont need anymore? Now for the actual context because all i did was describe a Linux distro, kinda but not really. Linux needs all those dev essentials were talking about and special repo's and wget this and that. Now that you know what the base "dumb" deterministic kernel needs to be structured like, we figure out the other non-intelligent processes and services that have to work before we even say LLM or api clients. My thoughts once you figure out how to build the kernel, AsyncEventBus, AsyncEventBusProcesser, AsyncCommandLineInterface, KernelManger, KernelAPI, enums (i.e. Events, EventType, ModuleInfo, ModuleState, Priority, SystemState, QueueItem, and dataclasses, Priority Queue, ConfigManager, BaseModule and ModuleInterface with ModuleLoader discovery, register modules, handle config changes,etc, Diagnostics with health checks and system metrics and monitoring, , all modules can inherit from kernel or system basemodule for system critical modules, but i think that when we get to instlling and uninstaaling things, the module way won't work unless you're a dev who understand naming conventions and imports, also there should be 5 spec standard manifest structure for consistency, and maybe the base/core kernel class, i am thinking UnityKernel class, which is the kernel that the rest of the system deals with, all those others should be coded and set and done, UnityKernel will be the one that probably gets constantly changed when scaling this project or adding features, Also we need to provide the Event and EventType dataclass structures and with a .yaml or .json that can be eaily edited for adding and removing pub/sub events to the bus. If there isn't a registry with this in it then how does anything know what event to publish and what event to subscribe to? Also another thing I learned when building these things for the purpose that I intend is that asyncio and pub/sub allow for non blocking fire and forget parallel task processing, but an AI LLM enabled architecture needs fallbacls, redundancy, LLM just missed the published event so it didn't subscribe, now we got an error and a model still wondering why nothing is happening, this is where we actaully bring in redis and use it on top of this bus, redis pub/sub is cool but Redia Streams is where it's at, a contant meassge board that doesnt lose the published message so when that LLM hits snag and is running late, he can query streams or however it works and still respond, I also wonder and personally think Redis Streams would be perfect for capturing multi-modal data like sounds, voice, object detection, computer vision, bio-metrics, on board hardware and external connected devices that have large chaotic corpus's of data in different outputs and structures, Redis Streams captures everything on preset channels that can be accessed anytime. Think about trying to give a LLM eyes, touch and feel sensors, enterprise grade studio quality mic's that can pick up any type of sound, even the sounds humans can't hear, and possibly a camera cluster for vision and peripherals , all the scatter data and noise that would get brought in at once, would crash an AI model in 10 minutes. Everything gets streamed to Redis channels cleanly and the model can use best judgement for immediate context and what data to check in Streams. (If there is another software or something that can do what i just described, feel free to elaborate, or we can brainstorm custom edge case software, (after reading this, you will think I am a MIT engineer, I am not. i am a Master Electrician turned Startup Founder & CEO) I know Current Robots must have some type of multi modal sensor input processing on top of the PLC controllers, input) Now all that "physical world" stuff is me thinking ahead, the only thing LLM's are missing now is real physical space and perception of it's immediate environment. I want hooks for when i partner with Tesla for my version of LLM implementation and orchestration for robots, drones, driver-less 4 wheeler s, and other types of unmanned autonomous vehicles. Now back to the basics of what LLMs will need. LLM Providers and client managers, these type of architecture doesn't run on one model, it orchestrates several models that are used based on there respective specializations and strengths. See every build async systems but with an orchestraor that is a massive LLM, with a huge context window, but that shit breaks and isn't reproducible. The "dumb" deterministic async orchestration handles all that, models just plug and work. Which mean they need effective and efficient, redundant memory, I propose we use a combination of MongoDB, Weaviate, for vector emdeddings, semantics and compression. A model like Cohere Kimi-K2 is OpenSource and is probably more reliable then Cohere, but when we scale to hardware and physical infra, we need to make sure we're not using the external api(Chinese/English based model, this architecture is Proprietary IP and only USA infra is permitted to host or run, with government, DOD, Darpa, in the loop, Ill be damned if I build the always on AI architecture that would make Chinese models dangerous to America and our National Security)would shine in combination with these, plus use another instance of redis or valkey. This is my contribution to America winning the AI and Super Intelligence race. If Google met with me over my very first version of async software, then i know for sure that once this is working and reproducible, I will be signing DOD and National Security Contracts to deploy my infra for defensive strategic purposes. On the LLM and model subject, I even had this thought that for one, we don't need trillion parameter models for everything, a model with an IQ of 100 and structured hallucination free benchmarks will be sufficient enough to operate within the system. The trick is to use several smaller models that specialize in that use case for what ever there role may be in the system. A combination of smart, error free models orchestrated with a deterministic async parallel processing will be more capable and Intelligent as a whole system working flawlessly. And I even though about keeping the models outside the system. This architecture can still orchestrate without 10 models in one system, create a kernel/redis bridge adapter that they plug into for access to the system. This is super long and I feel like I am forgetting something important, but that;s Okay, this is enough for you to take and process and structure all of this into a usable system
